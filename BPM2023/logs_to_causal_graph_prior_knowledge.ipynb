{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d805fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import lingam\n",
    "from lingam.utils import make_dot\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "import networkx as nx\n",
    "import pydotplus\n",
    "import datetime\n",
    "\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "\n",
    "# process mining \n",
    "from pm4py.algo.discovery.alpha import algorithm as alpha_miner\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
    "from pm4py.algo.discovery.dfg import algorithm as dfg_discovery\n",
    "\n",
    "# viz\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "from pm4py.visualization.process_tree import visualizer as pt_visualizer\n",
    "from pm4py.visualization.heuristics_net import visualizer as hn_visualizer\n",
    "from pm4py.visualization.dfg import visualizer as dfg_visualization\n",
    "\n",
    "# misc \n",
    "from pm4py.objects.conversion.process_tree import converter as pt_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e5a9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as Xet\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec8b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09b9ac",
   "metadata": {},
   "source": [
    "###  LiNGAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78645bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run prior to actual algorithm runs - this is the definition of the alg\n",
    "import graphviz\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LassoLarsIC\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "import igraph as ig\n",
    "from scipy.special import expit as sigmoid\n",
    "import random\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"print_causal_directions\",\n",
    "    \"print_dagc\",\n",
    "    \"make_prior_knowledge\",\n",
    "    \"remove_effect\",\n",
    "    \"make_dot\",\n",
    "    \"predict_adaptive_lasso\",\n",
    "    \"get_sink_variables\",\n",
    "    \"get_exo_variables\",\n",
    "    \"find_all_paths\",\n",
    "    \"simulate_dag\",\n",
    "    \"simulate_parameter\",\n",
    "    \"simulate_linear_sem\",\n",
    "    \"count_accuracy\",\n",
    "    \"set_random_seed\",\n",
    "]\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def simulate_linear_sem(adjacency_matrix, n_samples, sem_type, noise_scale=1.0):\n",
    "    \"\"\"Simulate samples from linear SEM with specified type of noise.\n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency_matrix : array-like, shape (n_features, n_features)\n",
    "        Weighted adjacency matrix of DAG, where ``n_features``\n",
    "        is the number of variables.\n",
    "    n_samples : int\n",
    "        Number of samples. n_samples=inf mimics population risk.\n",
    "    sem_type : str\n",
    "        SEM type. gauss, exp, gumbel, logistic, poisson.\n",
    "    noise_scale : float\n",
    "        scale parameter of additive noise.\n",
    "    Returns\n",
    "    -------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Data generated from linear SEM with specified type of noise,\n",
    "        where ``n_features`` is the number of variables.\n",
    "    \"\"\"\n",
    "    def _simulate_single_equation(X, w):\n",
    "        \"\"\"Simulate samples from a single equation.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features_parents)\n",
    "            Data of parents for a specified variable, where\n",
    "            n_features_parents is the number of parents.\n",
    "        w : array-like, shape (1, n_features_parents)\n",
    "            Weights of parents.\n",
    "        Returns\n",
    "        -------\n",
    "        x : array-like, shape (n_samples, 1)\n",
    "            Data for the specified variable.\n",
    "        \"\"\"\n",
    "        if sem_type == 'gauss':\n",
    "            z = np.random.normal(scale=noise_scale, size=n_samples)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'exp':\n",
    "            z = np.random.exponential(scale=noise_scale, size=n_samples)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'gumbel':\n",
    "            z = np.random.gumbel(scale=noise_scale, size=n_samples)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'logistic':\n",
    "            x = np.random.binomial(1, sigmoid(X @ w)) * 1.0\n",
    "        elif sem_type == 'poisson':\n",
    "            x = np.random.poisson(np.exp(X @ w)) * 1.0\n",
    "        elif sem_type == 'subGaussian':\n",
    "            z = np.random.normal(scale=noise_scale, size=n_samples)\n",
    "            q = 0.5 + 0.3 * np.random.rand(1)  # sub-Gaussian\n",
    "            z = np.sign(z) * pow(np.abs(z), q)\n",
    "            z = z - np.mean(z)\n",
    "            z = z / np.std(z)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'supGaussian':\n",
    "            z = np.random.normal(scale=noise_scale, size=n_samples)\n",
    "            q = 1.2 + 0.8 * np.random.rand(1)  # super-Gaussian\n",
    "            z = np.sign(z) * pow(np.abs(z), q)\n",
    "            z = z - np.mean(z)\n",
    "            z = z / np.std(z)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'nonGaussian':\n",
    "            z = np.random.normal(scale=noise_scale, size=n_samples)\n",
    "            qq = -1\n",
    "            if qq == 1:\n",
    "                q = 0.5 + 0.3 * np.random.rand(1)  # sub-Gaussian\n",
    "            else:\n",
    "                q = 1.2 + 0.8 * np.random.rand(1)  # super-Gaussian\n",
    "            z = np.sign(z) * pow(np.abs(z), q)\n",
    "            z = z - np.mean(z)\n",
    "            z = z / np.std(z)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'uniform':\n",
    "            z = np.random.uniform(0, 1, n_samples)\n",
    "            z = z - np.mean(z)\n",
    "            z = z / np.std(z)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'gamma':\n",
    "            z = np.random.gamma(2, 2, n_samples)\n",
    "            z = z - np.mean(z)\n",
    "            z = z / np.std(z)\n",
    "            x = X @ w + z\n",
    "        elif sem_type == 'laplace':\n",
    "            z = np.random.laplace(0, scale=noise_scale, size=n_samples)\n",
    "            x = X @ w + z\n",
    "        else:\n",
    "            raise ValueError('unknown sem type')\n",
    "        return x\n",
    "\n",
    "    n_features = adjacency_matrix.shape[0]\n",
    "    if np.isinf(n_samples):\n",
    "        if sem_type == 'gauss':\n",
    "            # make 1/n_features X'X = true cov\n",
    "            X = np.sqrt(n_features) * noise_scale * np.linalg.pinv(np.eye(n_features) - adjacency_matrix)\n",
    "            return X\n",
    "        else:\n",
    "            raise ValueError('population risk not available')\n",
    "    X = np.zeros([n_samples, n_features])\n",
    "\n",
    "    G = ig.Graph.Weighted_Adjacency(adjacency_matrix.tolist())\n",
    "    ordered_vertices = G.topological_sorting()\n",
    "    assert len(ordered_vertices) == n_features\n",
    "\n",
    "    for j in ordered_vertices:\n",
    "        parents = G.neighbors(j, mode=ig.IN)\n",
    "        X[:, j] = _simulate_single_equation(X[:, parents], adjacency_matrix[parents, j])\n",
    "    return X\n",
    "\n",
    "\n",
    "def count_accuracy(W_true, W, W_und=None):\n",
    "    \"\"\"Compute recalls and precisions for W, or optionally for CPDAG = W + W_und.\n",
    "    Parameters\n",
    "    ----------\n",
    "    W_true : array-like, shape (n_features, n_features)\n",
    "        Ground truth graph, where ``n_features`` is\n",
    "        the number of features.\n",
    "    W : array-like, shape (n_features, n_features)\n",
    "        Predicted graph.\n",
    "    W_und : array-like, shape (n_features, n_features)\n",
    "        Predicted undirected edges in CPDAG, asymmetric.\n",
    "    Returns\n",
    "    -------\n",
    "    recall : float\n",
    "        (true positive) / (true positive + false negative).\n",
    "    precision : float\n",
    "        (true positive) / (true positive + false positive).\n",
    "    \"\"\"\n",
    "    # convert to binary adjacency matrix\n",
    "    B_true = (W_true != 0)\n",
    "    B = (W != 0)\n",
    "    B_und = None if W_und is None else (W_und != 0)\n",
    "    # linear index of nonzeros\n",
    "    pred_und = None\n",
    "    if B_und is not None:\n",
    "        pred_und = np.flatnonzero(B_und)\n",
    "    pred = np.flatnonzero(B)\n",
    "    cond = np.flatnonzero(B_true)\n",
    "    cond_reversed = np.flatnonzero(B_true.T)\n",
    "    cond_skeleton = np.concatenate([cond, cond_reversed])\n",
    "    # true pos\n",
    "    true_pos = np.intersect1d(pred, cond, assume_unique=True)\n",
    "    if B_und is not None:\n",
    "        # treat undirected edge favorably\n",
    "        true_pos_und = np.intersect1d(pred_und, cond_skeleton, assume_unique=True)\n",
    "        true_pos = np.concatenate([true_pos, true_pos_und])\n",
    "    # false pos\n",
    "    false_pos = np.setdiff1d(pred, cond_skeleton, assume_unique=True)\n",
    "    if B_und is not None:\n",
    "        false_pos_und = np.setdiff1d(pred_und, cond_skeleton, assume_unique=True)\n",
    "        false_pos = np.concatenate([false_pos, false_pos_und])\n",
    "    # reverse\n",
    "    # extra = np.setdiff1d(pred, cond, assume_unique=True)\n",
    "    # compute ratio\n",
    "    pred_size = len(pred)\n",
    "    if B_und is not None:\n",
    "        pred_size += len(pred_und)\n",
    "    # fdr = float(len(reverse) + len(false_pos)) / max(pred_size, 1)\n",
    "    tpr = float(len(true_pos)) / max(len(cond), 1)\n",
    "    # fpr = float(len(reverse) + len(false_pos)) / max(cond_neg_size, 1)\n",
    "\n",
    "    recall = tpr\n",
    "    precision = float(len(true_pos)) / max(pred_size, 1)\n",
    "\n",
    "    return recall, precision\n",
    "\n",
    "\n",
    "def simulate_parameter(B, w_ranges=((-2.0, -0.5), (0.5, 2.0))):\n",
    "    \"\"\"Simulate SEM parameters for a DAG.\n",
    "    Parameters\n",
    "    ----------\n",
    "    B : array-like, shape (n_features, n_features)\n",
    "        Binary adjacency matrix of DAG, where ``n_features``\n",
    "        is the number of features.\n",
    "    w_ranges : tuple\n",
    "        Disjoint weight ranges.\n",
    "    Returns\n",
    "    -------\n",
    "    adjacency_matrix : array-like, shape (n_features, n_features)\n",
    "        Weighted adj matrix of DAG, where ``n_features``\n",
    "        is the number of features.\n",
    "    \"\"\"\n",
    "\n",
    "    adjacency_matrix = np.zeros(B.shape)\n",
    "    S = np.random.randint(len(w_ranges), size=B.shape)  # which range\n",
    "    for i, (low, high) in enumerate(w_ranges):\n",
    "        U = np.random.uniform(low=low, high=high, size=B.shape)\n",
    "        adjacency_matrix += B * (S == i) * U\n",
    "    return adjacency_matrix\n",
    "\n",
    "\n",
    "def simulate_dag(n_features, n_edges, graph_type):\n",
    "    \"\"\"Simulate random DAG with some expected number of edges.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_features : int\n",
    "        Number of features.\n",
    "    n_edges : int\n",
    "        Expected number of edges.\n",
    "    graph_type : str\n",
    "        ER, SF.\n",
    "    Returns\n",
    "    -------\n",
    "    B : array-like, shape (n_features, n_features)\n",
    "        binary adjacency matrix of DAG.\n",
    "    \"\"\"\n",
    "    def _random_permutation(M):\n",
    "        # np.random.permutation permutes first axis only\n",
    "        P = np.random.permutation(np.eye(M.shape[0]))\n",
    "        return P.T @ M @ P\n",
    "\n",
    "    def _random_acyclic_orientation(B_und):\n",
    "        return np.tril(_random_permutation(B_und), k=-1)\n",
    "\n",
    "    def _graph_to_adjmat(G):\n",
    "        return np.array(G.get_adjacency().data)\n",
    "\n",
    "    if graph_type == 'ER':\n",
    "        # Erdos-Renyi\n",
    "        G_und = ig.Graph.Erdos_Renyi(n=n_features, m=n_edges)\n",
    "        B_und = _graph_to_adjmat(G_und)\n",
    "        B = _random_acyclic_orientation(B_und)\n",
    "    elif graph_type == 'SF':\n",
    "        # Scale-free, Barabasi-Albert\n",
    "        G = ig.Graph.Barabasi(n=n_features, m=int(round(n_edges / n_features)), directed=True)\n",
    "        B = _graph_to_adjmat(G)\n",
    "    elif graph_type == 'BP':\n",
    "        # Bipartite, Sec 4.1 of (Gu, Fu, Zhou, 2018)\n",
    "        top = int(0.2 * n_features)\n",
    "        G = ig.Graph.Random_Bipartite(top, n_features - top, m=n_edges, directed=True, neimode=ig.OUT)\n",
    "        B = _graph_to_adjmat(G)\n",
    "    else:\n",
    "        raise ValueError('unknown graph type')\n",
    "    B_perm = _random_permutation(B)\n",
    "    assert ig.Graph.Adjacency(B_perm.tolist()).is_dag()\n",
    "    return B_perm\n",
    "\n",
    "\n",
    "def print_causal_directions(cdc, n_sampling, labels=None):\n",
    "    \"\"\"Print causal directions of bootstrap result to stdout.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cdc : dict\n",
    "        List of causal directions sorted by count in descending order.\n",
    "        This can be set the value returned by ``BootstrapResult.get_causal_direction_counts()`` method.\n",
    "    n_sampling : int\n",
    "        Number of bootstrapping samples.\n",
    "    labels : array-like, optional (default=None)\n",
    "        List of feature lables.\n",
    "        If set labels, the output feature name will be the specified label.\n",
    "    \"\"\"\n",
    "    for i, (fr, to, co) in enumerate(zip(cdc[\"from\"], cdc[\"to\"], cdc[\"count\"])):\n",
    "        sign = \"\" if \"sign\" not in cdc else \"(b>0)\" if cdc[\"sign\"][i] > 0 else \"(b<0)\"\n",
    "        if labels:\n",
    "            print(f\"{labels[to]} <--- {labels[fr]} {sign} ({100*co/n_sampling:.1f}%)\")\n",
    "        else:\n",
    "            print(f\"x{to} <--- x{fr} {sign} ({100*co/n_sampling:.1f}%)\")\n",
    "\n",
    "\n",
    "def print_dagc(dagc, n_sampling, labels=None):\n",
    "    \"\"\"Print DAGs of bootstrap result to stdout.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dagc : dict\n",
    "        List of directed acyclic graphs sorted by count in descending order.\n",
    "        This can be set the value returned by ``BootstrapResult.get_directed_acyclic_graph_counts()`` method.\n",
    "    n_sampling : int\n",
    "        Number of bootstrapping samples.\n",
    "    labels : array-like, optional (default=None)\n",
    "        List of feature lables.\n",
    "        If set labels, the output feature name will be the specified label.\n",
    "    \"\"\"\n",
    "    for i, (dag, co) in enumerate(zip(dagc[\"dag\"], dagc[\"count\"])):\n",
    "        print(f\"DAG[{i}]: {100*co/n_sampling:.1f}%\")\n",
    "        for j, (fr, to) in enumerate(zip(dag[\"from\"], dag[\"to\"])):\n",
    "            sign = \"\" if \"sign\" not in dag else \"(b>0)\" if dag[\"sign\"][j] > 0 else \"(b<0)\"\n",
    "            if labels:\n",
    "                print(\"\\t\" + f\"{labels[to]} <--- {labels[fr]} {sign}\")\n",
    "            else:\n",
    "                print(\"\\t\" + f\"x{to} <--- x{fr} {sign}\")\n",
    "\n",
    "\n",
    "def make_prior_knowledge(\n",
    "    n_variables,\n",
    "    exogenous_variables=None,\n",
    "    sink_variables=None,\n",
    "    paths=None,\n",
    "    no_paths=None,\n",
    "):\n",
    "    \"\"\"Make matrix of prior knowledge.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_variables : int\n",
    "        Number of variables.\n",
    "    exogenous_variables : array-like, shape (index, ...), optional (default=None)\n",
    "        List of exogenous variables(index).\n",
    "        Prior knowledge is created with the specified variables as exogenous variables.\n",
    "    sink_variables : array-like, shape (index, ...), optional (default=None)\n",
    "        List of sink variables(index).\n",
    "        Prior knowledge is created with the specified variables as sink variables.\n",
    "    paths : array-like, shape ((index, index), ...), optional (default=None)\n",
    "        List of variables(index) pairs with directed path.\n",
    "        If ``(i, j)``, prior knowledge is created that xi has a directed path to xj.\n",
    "    no_paths : array-like, shape ((index, index), ...), optional (default=None)\n",
    "        List of variables(index) pairs without directed path.\n",
    "        If ``(i, j)``, prior knowledge is created that xi does not have a directed path to xj.\n",
    "    Returns\n",
    "    -------\n",
    "    prior_knowledge : array-like, shape (n_variables, n_variables)\n",
    "        Return matrix of prior knowledge used for causal discovery.\n",
    "    \"\"\"\n",
    "    prior_knowledge = np.full((n_variables, n_variables), -1)\n",
    "    if no_paths:\n",
    "        for no_path in no_paths:\n",
    "            prior_knowledge[no_path[1], no_path[0]] = 0\n",
    "    if paths:\n",
    "        for path in paths:\n",
    "            prior_knowledge[path[1], path[0]] = 1\n",
    "    if sink_variables:\n",
    "        for var in sink_variables:\n",
    "            prior_knowledge[:, var] = 0\n",
    "    if exogenous_variables:\n",
    "        for var in exogenous_variables:\n",
    "            prior_knowledge[var, :] = 0\n",
    "    np.fill_diagonal(prior_knowledge, -1)\n",
    "    return prior_knowledge\n",
    "\n",
    "\n",
    "def get_sink_variables(adjacency_matrix):\n",
    "    \"\"\"The sink variables(index) in the adjacency matrix.\n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency_matrix : array-like, shape (n_variables, n_variables)\n",
    "        Adjacency matrix, where n_variables is the number of variables.\n",
    "    Returns\n",
    "    -------\n",
    "    sink_variables : array-like\n",
    "        List of sink variables(index).\n",
    "    \"\"\"\n",
    "    am = adjacency_matrix.copy()\n",
    "    am = np.abs(am)\n",
    "    np.fill_diagonal(am, 0)\n",
    "    sink_vars = [i for i in range(am.shape[1]) if am[:, i].sum() == 0]\n",
    "    return sink_vars\n",
    "\n",
    "\n",
    "def get_exo_variables(adjacency_matrix):\n",
    "    \"\"\"The exogenous variables(index) in the adjacency matrix.\n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency_matrix : array-like, shape (n_variables, n_variables)\n",
    "        Adjacency matrix, where n_variables is the number of variables.\n",
    "    Returns\n",
    "    -------\n",
    "    exogenous_variables : array-like\n",
    "        List of exogenous variables(index).\n",
    "    \"\"\"\n",
    "    am = adjacency_matrix.copy()\n",
    "    am = np.abs(am)\n",
    "    np.fill_diagonal(am, 0)\n",
    "    exo_vars = [i for i in range(am.shape[1]) if am[i, :].sum() == 0]\n",
    "    return exo_vars\n",
    "\n",
    "\n",
    "def remove_effect(X, remove_features):\n",
    "    \"\"\"Create a dataset that removes the effects of features by linear regression.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Data, where ``n_samples`` is the number of samples\n",
    "        and ``n_features`` is the number of features.\n",
    "    remove_features : array-like\n",
    "        List of features(index) to remove effects.\n",
    "    Returns\n",
    "    -------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Data after removing effects of ``remove_features``.\n",
    "    \"\"\"\n",
    "    X = np.copy(check_array(X))\n",
    "    features_ = [i for i in np.arange(X.shape[1]) if i not in remove_features]\n",
    "    for feature in features_:\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(X[:, remove_features], X[:, feature])\n",
    "        X[:, feature] = X[:, feature] - reg.predict(X[:, remove_features])\n",
    "    return X\n",
    "\n",
    "\n",
    "def make_dot(\n",
    "    adjacency_matrix,\n",
    "    labels=None,\n",
    "    lower_limit=0.01,\n",
    "    prediction_feature_indices=None,\n",
    "    prediction_target_label=\"Y(pred)\",\n",
    "    prediction_line_color=\"red\",\n",
    "    prediction_coefs=None,\n",
    "    prediction_feature_importance=None,\n",
    "    ignore_shape=False,\n",
    "):\n",
    "    \"\"\"Directed graph source code in the DOT language with specified adjacency matrix.\n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency_matrix : array-like with shape (n_features, n_features)\n",
    "        Adjacency matrix to make graph, where ``n_features`` is the number of features.\n",
    "    labels : array-like, optional (default=None)\n",
    "        Label to use for graph features.\n",
    "    lower_limit : float, optional (default=0.01)\n",
    "        Threshold for drawing direction.\n",
    "        If float, then directions with absolute values of coefficients less than ``lower_limit`` are excluded.\n",
    "    prediction_feature_indices : array-like, optional (default=None)\n",
    "        Indices to use as prediction features.\n",
    "    prediction_target_label : string, optional (default='Y(pred)'))\n",
    "        Label to use for target variable of prediction.\n",
    "    prediction_line_color : string, optional (default='red')\n",
    "        Line color to use for prediction's graph.\n",
    "    prediction_coefs : array-like, optional (default=None)\n",
    "        Coefficients to use for prediction's graph.\n",
    "    prediction_feature_importance : array-like, optional (default=None)\n",
    "        Feature importance to use for prediction's graph.\n",
    "    ignore_shape : boolean, optional (default=False)\n",
    "        Ignore checking the shape of adjaceny_matrix or not.\n",
    "    Returns\n",
    "    -------\n",
    "    graph : graphviz.Digraph\n",
    "        Directed graph source code in the DOT language.\n",
    "        If order is unknown, draw a double-headed arrow.\n",
    "    \"\"\"\n",
    "    # Check parameters\n",
    "    B = check_array(np.nan_to_num(adjacency_matrix))\n",
    "    if not ignore_shape and B.shape[0] != B.shape[1]:\n",
    "        raise ValueError(\"'adjacency_matrix' is not square matrix.\")\n",
    "    if labels is not None:\n",
    "        if B.shape[1] != len(labels):\n",
    "            raise ValueError(\n",
    "                \"Length of 'labels' does not match length of 'adjacency_matrix'\"\n",
    "            )\n",
    "    if prediction_feature_indices is not None:\n",
    "        if prediction_coefs is not None and (\n",
    "            len(prediction_feature_indices) != len(prediction_coefs)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Length of 'prediction_coefs' does not match length of 'prediction_feature_indices'\"\n",
    "            )\n",
    "        if prediction_feature_importance is not None and (\n",
    "            len(prediction_feature_indices) != len(prediction_feature_importance)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Length of 'prediction_feature_importance' does not match length of 'prediction_feature_indices'\"\n",
    "            )\n",
    "\n",
    "    d = graphviz.Digraph(engine=\"dot\")\n",
    "\n",
    "    # nodes\n",
    "    names = labels if labels else [f\"x{i}\" for i in range(len(B))]\n",
    "    for name in names:\n",
    "        d.node(name)\n",
    "\n",
    "    # edges\n",
    "    idx = np.abs(B) > lower_limit\n",
    "    dirs = np.where(idx)\n",
    "    for to, from_, coef in zip(dirs[0], dirs[1], B[idx]):\n",
    "        d.edge(names[from_], names[to], label=f\"{coef:.2f}\")\n",
    "\n",
    "    # integrate of prediction model\n",
    "    if prediction_feature_indices is not None:\n",
    "        d.node(\n",
    "            prediction_target_label,\n",
    "            color=prediction_line_color,\n",
    "            fontcolor=prediction_line_color,\n",
    "        )\n",
    "\n",
    "        if prediction_coefs is not None:\n",
    "            for from_, coef in zip(prediction_feature_indices, prediction_coefs):\n",
    "                if np.abs(coef) > lower_limit:\n",
    "                    d.edge(\n",
    "                        names[from_],\n",
    "                        prediction_target_label,\n",
    "                        label=f\"{coef:.2f}\",\n",
    "                        color=prediction_line_color,\n",
    "                        fontcolor=prediction_line_color,\n",
    "                        style=\"dashed\",\n",
    "                    )\n",
    "\n",
    "        elif prediction_feature_importance is not None:\n",
    "            for from_, imp in zip(\n",
    "                prediction_feature_indices, prediction_feature_importance\n",
    "            ):\n",
    "                d.edge(\n",
    "                    names[from_],\n",
    "                    prediction_target_label,\n",
    "                    label=f\"({imp})\",\n",
    "                    color=prediction_line_color,\n",
    "                    fontcolor=prediction_line_color,\n",
    "                    style=\"dashed\",\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            for from_ in prediction_feature_indices:\n",
    "                d.edge(\n",
    "                    names[from_],\n",
    "                    prediction_target_label,\n",
    "                    color=prediction_line_color,\n",
    "                    style=\"dashed\",\n",
    "                )\n",
    "\n",
    "    # If the value is nan, draw a double-headed arrow\n",
    "    unk_order = np.where(np.isnan(np.tril(adjacency_matrix)))\n",
    "    unk_order_set = set([val for item in unk_order for val in item])\n",
    "    with d.subgraph() as s:\n",
    "        s.attr(rank=\"same\")\n",
    "        for node in unk_order_set:\n",
    "            s.node(names[node])\n",
    "    for to, from_ in zip(unk_order[0], unk_order[1]):\n",
    "        d.edge(names[from_], names[to], dir=\"both\")\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def predict_adaptive_lasso(X, predictors, target, gamma=1.0):\n",
    "    \"\"\"Predict with Adaptive Lasso.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training data, where n_samples is the number of samples\n",
    "        and n_features is the number of features.\n",
    "    predictors : array-like, shape (n_predictors)\n",
    "        Indices of predictor variable.\n",
    "    target : int\n",
    "        Index of target variable.\n",
    "    Returns\n",
    "    -------\n",
    "    coef : array-like, shape (n_features)\n",
    "        Coefficients of predictor variable.\n",
    "    \"\"\"\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X[:, predictors], X[:, target])\n",
    "    weight = np.power(np.abs(lr.coef_), gamma)\n",
    "    reg = LassoLarsIC(criterion=\"bic\",positive=True)\n",
    "    reg.fit(X[:, predictors] * weight, X[:, target])\n",
    "    return reg.coef_ * weight\n",
    "\n",
    "\n",
    "def find_all_paths(dag, from_index, to_index, min_causal_effect=0.0):\n",
    "    \"\"\"Find all paths from point to point in DAG.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dag : array-like, shape (n_features, n_features)\n",
    "        The adjacency matrix to fine all paths, where n_features is the number of features.\n",
    "    from_index : int\n",
    "        Index of the variable at the start of the path.\n",
    "    to_index : int\n",
    "        Index of the variable at the end of the path.\n",
    "    min_causal_effect : float, optional (default=0.0)\n",
    "        Threshold for detecting causal direction.\n",
    "        Causal directions with absolute values of causal effects less than ``min_causal_effect`` are excluded.\n",
    "    Returns\n",
    "    -------\n",
    "    paths : array-like, shape (n_paths)\n",
    "        List of found path, where n_paths is the number of paths.\n",
    "    effects : array-like, shape (n_paths)\n",
    "        List of causal effect, where n_paths is the number of paths.\n",
    "    \"\"\"\n",
    "    # Extract all edges\n",
    "    edges = np.array(np.where(np.abs(np.nan_to_num(dag)) > min_causal_effect)).T\n",
    "\n",
    "    # Aggregate edges by start point\n",
    "    to_indices = []\n",
    "    for i in range(dag.shape[0]):\n",
    "        adj_list = edges[edges[:, 1] == i][:, 0].tolist()\n",
    "        if len(adj_list) != 0:\n",
    "            to_indices.append(adj_list)\n",
    "        else:\n",
    "            to_indices.append([])\n",
    "\n",
    "    # DFS\n",
    "    paths = []\n",
    "    stack = [from_index]\n",
    "    stack_to_indice = [to_indices[from_index]]\n",
    "    while stack:\n",
    "        if len(stack) > dag.shape[0]:\n",
    "            raise ValueError(\n",
    "                \"Unable to find the path because a cyclic graph has been specified.\"\n",
    "            )\n",
    "\n",
    "        cur_index = stack[-1]\n",
    "        to_indice = stack_to_indice[-1]\n",
    "\n",
    "        if cur_index == to_index:\n",
    "            paths.append(stack.copy())\n",
    "            stack.pop()\n",
    "            stack_to_indice.pop()\n",
    "        else:\n",
    "            if len(to_indice) > 0:\n",
    "                next_index = to_indice.pop(0)\n",
    "                stack.append(next_index)\n",
    "                stack_to_indice.append(to_indices[next_index].copy())\n",
    "            else:\n",
    "                stack.pop()\n",
    "                stack_to_indice.pop()\n",
    "\n",
    "    # Calculate the causal effect for each path\n",
    "    effects = []\n",
    "    for p in paths:\n",
    "        coefs = [dag[p[i + 1], p[i]] for i in range(len(p) - 1)]\n",
    "        effects.append(np.cumprod(coefs)[-1])\n",
    "\n",
    "    return paths, effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca44e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python implementation of the LiNGAM algorithms.\n",
    "The LiNGAM Project: https://sites.google.com/site/sshimizu06/lingam\n",
    "\"\"\"\n",
    "#Run after the previous block\n",
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import check_array, resample\n",
    "\n",
    "\n",
    "class BootstrapMixin:\n",
    "    \"\"\"Mixin class for all LiNGAM algorithms that implement the method of bootstrapping.\"\"\"\n",
    "\n",
    "    def bootstrap(self, X, n_sampling):\n",
    "        \"\"\"Evaluate the statistical reliability of DAG based on the bootstrapping.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where ``n_samples`` is the number of samples\n",
    "            and ``n_features`` is the number of features.\n",
    "        n_sampling : int\n",
    "            Number of bootstrapping samples.\n",
    "        Returns\n",
    "        -------\n",
    "        result : BootstrapResult\n",
    "            Returns the result of bootstrapping.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        X = check_array(X)\n",
    "\n",
    "        if isinstance(n_sampling, (numbers.Integral, np.integer)):\n",
    "            if not 0 < n_sampling:\n",
    "                raise ValueError(\"n_sampling must be an integer greater than 0.\")\n",
    "        else:\n",
    "            raise ValueError(\"n_sampling must be an integer greater than 0.\")\n",
    "\n",
    "        # Bootstrapping\n",
    "        adjacency_matrices = np.zeros([n_sampling, X.shape[1], X.shape[1]])\n",
    "        total_effects = np.zeros([n_sampling, X.shape[1], X.shape[1]])\n",
    "        for i in range(n_sampling):\n",
    "            self.fit(resample(X))\n",
    "            adjacency_matrices[i] = self._adjacency_matrix\n",
    "\n",
    "            # Calculate total effects\n",
    "            for c, from_ in enumerate(self._causal_order):\n",
    "                for to in self._causal_order[c + 1 :]:\n",
    "                    total_effects[i, to, from_] = self.estimate_total_effect(\n",
    "                        X, from_, to\n",
    "                    )\n",
    "\n",
    "        return BootstrapResult(adjacency_matrices, total_effects)\n",
    "\n",
    "\n",
    "class BootstrapResult(object):\n",
    "    \"\"\"The result of bootstrapping.\"\"\"\n",
    "\n",
    "    def __init__(self, adjacency_matrices, total_effects):\n",
    "        \"\"\"Construct a BootstrapResult.\n",
    "        Parameters\n",
    "        ----------\n",
    "        adjacency_matrices : array-like, shape (n_sampling)\n",
    "            The adjacency matrix list by bootstrapping.\n",
    "        total_effects : array-like, shape (n_sampling)\n",
    "            The total effects list by bootstrapping.\n",
    "        \"\"\"\n",
    "        self._adjacency_matrices = adjacency_matrices\n",
    "        self._total_effects = total_effects\n",
    "\n",
    "    @property\n",
    "    def adjacency_matrices_(self):\n",
    "        \"\"\"The adjacency matrix list by bootstrapping.\n",
    "        Returns\n",
    "        -------\n",
    "        adjacency_matrices_ : array-like, shape (n_sampling)\n",
    "            The adjacency matrix list, where ``n_sampling`` is\n",
    "            the number of bootstrap sampling.\n",
    "        \"\"\"\n",
    "        return self._adjacency_matrices\n",
    "\n",
    "    @property\n",
    "    def total_effects_(self):\n",
    "        \"\"\"The total effect list by bootstrapping.\n",
    "        Returns\n",
    "        -------\n",
    "        total_effects_ : array-like, shape (n_sampling)\n",
    "            The total effect list, where ``n_sampling`` is\n",
    "            the number of bootstrap sampling.\n",
    "        \"\"\"\n",
    "        return self._total_effects\n",
    "\n",
    "    def get_causal_direction_counts(\n",
    "        self,\n",
    "        n_directions=None,\n",
    "        min_causal_effect=None,\n",
    "        split_by_causal_effect_sign=False,\n",
    "    ):\n",
    "        \"\"\"Get causal direction count as a result of bootstrapping.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_directions : int, optional (default=None)\n",
    "            If int, then The top ``n_directions`` items are included in the result\n",
    "        min_causal_effect : float, optional (default=None)\n",
    "            Threshold for detecting causal direction.\n",
    "            If float, then causal directions with absolute values of causal effects\n",
    "            less than ``min_causal_effect`` are excluded.\n",
    "        split_by_causal_effect_sign : boolean, optional (default=False)\n",
    "            If True, then causal directions are split depending on the sign of the causal effect.\n",
    "        Returns\n",
    "        -------\n",
    "        causal_direction_counts : dict\n",
    "            List of causal directions sorted by count in descending order.\n",
    "            The dictionary has the following format::\n",
    "            {'from': [n_directions], 'to': [n_directions], 'count': [n_directions]}\n",
    "            where ``n_directions`` is the number of causal directions.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        if isinstance(n_directions, (numbers.Integral, np.integer)):\n",
    "            if not 0 < n_directions:\n",
    "                raise ValueError(\"n_directions must be an integer greater than 0\")\n",
    "        elif n_directions is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"n_directions must be an integer greater than 0\")\n",
    "\n",
    "        if min_causal_effect is None:\n",
    "            min_causal_effect = 0.0\n",
    "        else:\n",
    "            if not 0.0 < min_causal_effect:\n",
    "                raise ValueError(\"min_causal_effect must be an value greater than 0.\")\n",
    "\n",
    "        # Count causal directions\n",
    "        directions = []\n",
    "        for am in np.nan_to_num(self._adjacency_matrices):\n",
    "            direction = np.array(np.where(np.abs(am) > min_causal_effect))\n",
    "            if split_by_causal_effect_sign:\n",
    "                signs = (\n",
    "                    np.array([np.sign(am[i][j]) for i, j in direction.T])\n",
    "                    .astype(\"int64\")\n",
    "                    .T\n",
    "                )\n",
    "                direction = np.vstack([direction, signs])\n",
    "            directions.append(direction.T)\n",
    "        directions = np.concatenate(directions)\n",
    "\n",
    "        if len(directions) == 0:\n",
    "            cdc = {\"from\": [], \"to\": [], \"count\": []}\n",
    "            if split_by_causal_effect_sign:\n",
    "                cdc[\"sign\"] = []\n",
    "            return cdc\n",
    "\n",
    "        directions, counts = np.unique(directions, axis=0, return_counts=True)\n",
    "        sort_order = np.argsort(-counts)\n",
    "        sort_order = (\n",
    "            sort_order[:n_directions] if n_directions is not None else sort_order\n",
    "        )\n",
    "        counts = counts[sort_order]\n",
    "        directions = directions[sort_order]\n",
    "\n",
    "        cdc = {\n",
    "            \"from\": directions[:, 1].tolist(),\n",
    "            \"to\": directions[:, 0].tolist(),\n",
    "            \"count\": counts.tolist(),\n",
    "        }\n",
    "        if split_by_causal_effect_sign:\n",
    "            cdc[\"sign\"] = directions[:, 2].tolist()\n",
    "\n",
    "        return cdc\n",
    "\n",
    "    def get_directed_acyclic_graph_counts(\n",
    "        self, n_dags=None, min_causal_effect=None, split_by_causal_effect_sign=False\n",
    "    ):\n",
    "        \"\"\"Get DAGs count as a result of bootstrapping.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_dags : int, optional (default=None)\n",
    "            If int, then The top ``n_dags`` items are included in the result\n",
    "        min_causal_effect : float, optional (default=None)\n",
    "            Threshold for detecting causal direction.\n",
    "            If float, then causal directions with absolute values of causal effects less than\n",
    "            ``min_causal_effect`` are excluded.\n",
    "        split_by_causal_effect_sign : boolean, optional (default=False)\n",
    "            If True, then causal directions are split depending on the sign of the causal effect.\n",
    "        Returns\n",
    "        -------\n",
    "        directed_acyclic_graph_counts : dict\n",
    "            List of directed acyclic graphs sorted by count in descending order.\n",
    "            The dictionary has the following format::\n",
    "            {'dag': [n_dags], 'count': [n_dags]}.\n",
    "            where ``n_dags`` is the number of directed acyclic graphs.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        if isinstance(n_dags, (numbers.Integral, np.integer)):\n",
    "            if not 0 < n_dags:\n",
    "                raise ValueError(\"n_dags must be an integer greater than 0\")\n",
    "        elif n_dags is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"n_dags must be an integer greater than 0\")\n",
    "\n",
    "        if min_causal_effect is None:\n",
    "            min_causal_effect = 0.0\n",
    "        else:\n",
    "            if not 0.0 < min_causal_effect:\n",
    "                raise ValueError(\"min_causal_effect must be an value greater than 0.\")\n",
    "\n",
    "        # Count directed acyclic graphs\n",
    "        dags = []\n",
    "        for am in np.nan_to_num(self._adjacency_matrices):\n",
    "            dag = np.abs(am) > min_causal_effect\n",
    "            if split_by_causal_effect_sign:\n",
    "                direction = np.array(np.where(dag))\n",
    "                signs = np.zeros_like(dag).astype(\"int64\")\n",
    "                for i, j in direction.T:\n",
    "                    signs[i][j] = np.sign(am[i][j]).astype(\"int64\")\n",
    "                dag = signs\n",
    "            dags.append(dag)\n",
    "\n",
    "        dags, counts = np.unique(dags, axis=0, return_counts=True)\n",
    "        sort_order = np.argsort(-counts)\n",
    "        sort_order = sort_order[:n_dags] if n_dags is not None else sort_order\n",
    "        counts = counts[sort_order]\n",
    "        dags = dags[sort_order]\n",
    "\n",
    "        if split_by_causal_effect_sign:\n",
    "            dags = [\n",
    "                {\n",
    "                    \"from\": np.where(dag)[1].tolist(),\n",
    "                    \"to\": np.where(dag)[0].tolist(),\n",
    "                    \"sign\": [dag[i][j] for i, j in np.array(np.where(dag)).T],\n",
    "                }\n",
    "                for dag in dags\n",
    "            ]\n",
    "        else:\n",
    "            dags = [\n",
    "                {\"from\": np.where(dag)[1].tolist(), \"to\": np.where(dag)[0].tolist()}\n",
    "                for dag in dags\n",
    "            ]\n",
    "\n",
    "        return {\"dag\": dags, \"count\": counts.tolist()}\n",
    "\n",
    "    def get_probabilities(self, min_causal_effect=None):\n",
    "        \"\"\"Get bootstrap probability.\n",
    "        Parameters\n",
    "        ----------\n",
    "        min_causal_effect : float, optional (default=None)\n",
    "            Threshold for detecting causal direction.\n",
    "            If float, then causal directions with absolute values of causal effects less than\n",
    "            ``min_causal_effect`` are excluded.\n",
    "        Returns\n",
    "        -------\n",
    "        probabilities : array-like\n",
    "            List of bootstrap probability matrix.\n",
    "        \"\"\"\n",
    "        # check parameters\n",
    "        if min_causal_effect is None:\n",
    "            min_causal_effect = 0.0\n",
    "        else:\n",
    "            if not 0.0 < min_causal_effect:\n",
    "                raise ValueError(\"min_causal_effect must be an value greater than 0.\")\n",
    "\n",
    "        adjacency_matrices = np.nan_to_num(self._adjacency_matrices)\n",
    "        shape = adjacency_matrices[0].shape\n",
    "        bp = np.zeros(shape)\n",
    "        for B in adjacency_matrices:\n",
    "            bp += np.where(np.abs(B) > min_causal_effect, 1, 0)\n",
    "        bp = bp / len(adjacency_matrices)\n",
    "\n",
    "        if int(shape[1] / shape[0]) == 1:\n",
    "            return bp\n",
    "        else:\n",
    "            return np.hsplit(bp, int(shape[1] / shape[0]))\n",
    "\n",
    "    def get_total_causal_effects(self, min_causal_effect=None):\n",
    "        \"\"\"Get total effects list.\n",
    "        Parameters\n",
    "        ----------\n",
    "        min_causal_effect : float, optional (default=None)\n",
    "            Threshold for detecting causal direction.\n",
    "            If float, then causal directions with absolute values of causal effects less than\n",
    "            ``min_causal_effect`` are excluded.\n",
    "        Returns\n",
    "        -------\n",
    "        total_causal_effects : dict\n",
    "            List of bootstrap total causal effect sorted by probability in descending order.\n",
    "            The dictionary has the following format::\n",
    "            {'from': [n_directions], 'to': [n_directions], 'effect': [n_directions], 'probability': [n_directions]}\n",
    "            where ``n_directions`` is the number of causal directions.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        if min_causal_effect is None:\n",
    "            min_causal_effect = 0.0\n",
    "        else:\n",
    "            if not 0.0 < min_causal_effect:\n",
    "                raise ValueError(\"min_causal_effect must be an value greater than 0.\")\n",
    "\n",
    "        # Calculate probability\n",
    "        probs = np.sum(\n",
    "            np.where(np.abs(self._total_effects) > min_causal_effect, 1, 0),\n",
    "            axis=0,\n",
    "            keepdims=True,\n",
    "        )[0]\n",
    "        probs = probs / len(self._total_effects)\n",
    "\n",
    "        # Causal directions\n",
    "        dirs = np.array(np.where(np.abs(probs) > 0))\n",
    "        probs = probs[dirs[0], dirs[1]]\n",
    "\n",
    "        # Calculate median effect without zero\n",
    "        effects = np.zeros(dirs.shape[1])\n",
    "        for i, (to, from_) in enumerate(dirs.T):\n",
    "            idx = np.where(np.abs(self._total_effects[:, to, from_]) > 0)\n",
    "            effects[i] = np.median(self._total_effects[:, to, from_][idx])\n",
    "\n",
    "        # Sort by probability\n",
    "        order = np.argsort(-probs)\n",
    "        dirs = dirs.T[order]\n",
    "        effects = effects[order]\n",
    "        probs = probs[order]\n",
    "\n",
    "        ce = {\n",
    "            \"from\": dirs[:, 1].tolist(),\n",
    "            \"to\": dirs[:, 0].tolist(),\n",
    "            \"effect\": effects.tolist(),\n",
    "            \"probability\": probs.tolist(),\n",
    "        }\n",
    "\n",
    "        return ce\n",
    "\n",
    "    def get_paths(self, from_index, to_index, min_causal_effect=None):\n",
    "        \"\"\"Get all paths from the start variable to the end variable and their bootstrap probabilities.\n",
    "        Parameters\n",
    "        ----------\n",
    "        from_index : int\n",
    "            Index of the variable at the start of the path.\n",
    "        to_index : int\n",
    "            Index of the variable at the end of the path.\n",
    "        min_causal_effect : float, optional (default=None)\n",
    "            Threshold for detecting causal direction.\n",
    "            Causal directions with absolute values of causal effects less than ``min_causal_effect`` are excluded.\n",
    "        Returns\n",
    "        -------\n",
    "        paths : dict\n",
    "            List of path and bootstrap probability.\n",
    "            The dictionary has the following format::\n",
    "            {'path': [n_paths], 'effect': [n_paths], 'probability': [n_paths]}\n",
    "            where ``n_paths`` is the number of paths.\n",
    "        \"\"\"\n",
    "        # check parameters\n",
    "        if min_causal_effect is None:\n",
    "            min_causal_effect = 0.0\n",
    "        else:\n",
    "            if not 0.0 < min_causal_effect:\n",
    "                raise ValueError(\"min_causal_effect must be an value greater than 0.\")\n",
    "\n",
    "        # Find all paths from from_index to to_index\n",
    "        paths_list = []\n",
    "        effects_list = []\n",
    "        for am in self._adjacency_matrices:\n",
    "            paths, effects = find_all_paths(am, from_index, to_index, min_causal_effect)\n",
    "            # Convert path to string to make them easier to handle.\n",
    "            paths_list.extend([\"_\".join(map(str, p)) for p in paths])\n",
    "            effects_list.extend(effects)\n",
    "\n",
    "        paths_list = np.array(paths_list)\n",
    "        effects_list = np.array(effects_list)\n",
    "\n",
    "        # Count paths\n",
    "        paths_str, counts = np.unique(paths_list, axis=0, return_counts=True)\n",
    "\n",
    "        # Sort by count\n",
    "        order = np.argsort(-counts)\n",
    "        probs = counts[order] / len(self._adjacency_matrices)\n",
    "        paths_str = paths_str[order]\n",
    "\n",
    "        # Calculate median of causal effect for each path\n",
    "        effects = [\n",
    "            np.median(effects_list[np.where(paths_list == p)]) for p in paths_str\n",
    "        ]\n",
    "\n",
    "        result = {\n",
    "            \"path\": [[int(i) for i in p.split(\"_\")] for p in paths_str],\n",
    "            \"effect\": effects,\n",
    "            \"probability\": probs.tolist(),\n",
    "        }\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc337956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python implementation of the LiNGAM algorithms.\n",
    "The LiNGAM Project: https://sites.google.com/site/sshimizu06/lingam\n",
    "\"\"\"\n",
    "#Run after previous block\n",
    "import numpy as np\n",
    "from scipy.stats import gamma\n",
    "from statsmodels.nonparametric import bandwidths\n",
    "\n",
    "__all__ = [\"get_kernel_width\", \"get_gram_matrix\", \"hsic_teststat\", \"hsic_test_gamma\"]\n",
    "\n",
    "\n",
    "def get_kernel_width(X):\n",
    "    \"\"\"Calculate the bandwidth to median distance between points.\n",
    "    Use at most 100 points (since median is only a heuristic,\n",
    "    and 100 points is sufficient for a robust estimate).\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training data, where ``n_samples`` is the number of samples\n",
    "        and ``n_features`` is the number of features.\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The bandwidth parameter.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    if n_samples > 100:\n",
    "        X_med = X[:100, :]\n",
    "        n_samples = 100\n",
    "    else:\n",
    "        X_med = X\n",
    "\n",
    "    G = np.sum(X_med * X_med, 1).reshape(n_samples, 1)\n",
    "    Q = np.tile(G, (1, n_samples))\n",
    "    R = np.tile(G.T, (n_samples, 1))\n",
    "\n",
    "    dists = Q + R - 2 * np.dot(X_med, X_med.T)\n",
    "    dists = dists - np.tril(dists)\n",
    "    dists = dists.reshape(n_samples ** 2, 1)\n",
    "\n",
    "    return np.sqrt(0.5 * np.median(dists[dists > 0]))\n",
    "\n",
    "\n",
    "def _rbf_dot(X, Y, width):\n",
    "    \"\"\"Compute the inner product of radial basis functions.\"\"\"\n",
    "    n_samples_X = X.shape[0]\n",
    "    n_samples_Y = Y.shape[0]\n",
    "\n",
    "    G = np.sum(X * X, 1).reshape(n_samples_X, 1)\n",
    "    H = np.sum(Y * Y, 1).reshape(n_samples_Y, 1)\n",
    "    Q = np.tile(G, (1, n_samples_Y))\n",
    "    R = np.tile(H.T, (n_samples_X, 1))\n",
    "    H = Q + R - 2 * np.dot(X, Y.T)\n",
    "\n",
    "    return np.exp(-H / 2 / (width ** 2))\n",
    "\n",
    "\n",
    "def get_gram_matrix(X, width):\n",
    "    \"\"\"Get the centered gram matrices.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training data, where ``n_samples`` is the number of samples\n",
    "        and ``n_features`` is the number of features.\n",
    "    width : float\n",
    "        The bandwidth parameter.\n",
    "    Returns\n",
    "    -------\n",
    "    K, Kc : array\n",
    "        the centered gram matrices.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    H = np.eye(n) - 1 / n * np.ones((n, n))\n",
    "\n",
    "    K = _rbf_dot(X, X, width)\n",
    "    Kc = np.dot(np.dot(H, K), H)\n",
    "\n",
    "    return K, Kc\n",
    "\n",
    "\n",
    "def hsic_teststat(Kc, Lc, n):\n",
    "    \"\"\"get the HSIC statistic.\n",
    "    Parameters\n",
    "    ----------\n",
    "    K, Kc : array\n",
    "        the centered gram matrices.\n",
    "    n : float\n",
    "        the number of samples.\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        the HSIC statistic.\n",
    "    \"\"\"\n",
    "    # test statistic m*HSICb under H1\n",
    "    return 1 / n * np.sum(np.sum(Kc.T * Lc))\n",
    "\n",
    "\n",
    "def hsic_test_gamma(X, Y, bw_method=\"mdbs\"):\n",
    "    \"\"\"get the HSIC statistic.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, Y : array-like, shape (n_samples, n_features)\n",
    "        Training data, where ``n_samples`` is the number of samples\n",
    "        and ``n_features`` is the number of features.\n",
    "    bw_method : str, optional (default=``mdbs``)\n",
    "        The method used to calculate the bandwidth of the HSIC.\n",
    "        * ``mdbs`` : Median distance between samples.\n",
    "        * ``scott`` : Scott's Rule of Thumb.\n",
    "        * ``silverman`` : Silverman's Rule of Thumb.\n",
    "    Returns\n",
    "    -------\n",
    "    test_stat : float\n",
    "        the HSIC statistic.\n",
    "    p : float\n",
    "        the HSIC p-value.\n",
    "    \"\"\"\n",
    "    X = X.reshape(-1, 1) if X.ndim == 1 else X\n",
    "    Y = Y.reshape(-1, 1) if Y.ndim == 1 else Y\n",
    "\n",
    "    if bw_method == \"scott\":\n",
    "        width_x = bandwidths.bw_scott(X)\n",
    "        width_y = bandwidths.bw_scott(Y)\n",
    "    elif bw_method == \"silverman\":\n",
    "        width_x = bandwidths.bw_silverman(X)\n",
    "        width_y = bandwidths.bw_silverman(Y)\n",
    "    # Get kernel width to median distance between points\n",
    "    else:\n",
    "        width_x = get_kernel_width(X)\n",
    "        width_y = get_kernel_width(Y)\n",
    "\n",
    "    # these are slightly biased estimates of centered gram matrices\n",
    "    K, Kc = get_gram_matrix(X, width_x)\n",
    "    L, Lc = get_gram_matrix(Y, width_y)\n",
    "\n",
    "    # test statistic m*HSICb under H1\n",
    "    n = X.shape[0]\n",
    "    bone = np.ones((n, 1))\n",
    "    test_stat = hsic_teststat(Kc, Lc, n)\n",
    "\n",
    "    var = (1 / 6 * Kc * Lc) ** 2\n",
    "    # second subtracted term is bias correction\n",
    "    var = 1 / n / (n - 1) * (np.sum(np.sum(var)) - np.sum(np.diag(var)))\n",
    "    # variance under H0\n",
    "    var = 72 * (n - 4) * (n - 5) / n / (n - 1) / (n - 2) / (n - 3) * var\n",
    "\n",
    "    K = K - np.diag(np.diag(K))\n",
    "    L = L - np.diag(np.diag(L))\n",
    "    mu_X = 1 / n / (n - 1) * np.dot(bone.T, np.dot(K, bone))\n",
    "    mu_Y = 1 / n / (n - 1) * np.dot(bone.T, np.dot(L, bone))\n",
    "    # mean under H0\n",
    "    mean = 1 / n * (1 + mu_X * mu_Y - mu_X - mu_Y)\n",
    "\n",
    "    alpha = mean ** 2 / var\n",
    "    # threshold for hsicArr*m\n",
    "    beta = np.dot(var, n) / mean\n",
    "    p = 1 - gamma.cdf(test_stat, alpha, scale=beta)[0][0]\n",
    "\n",
    "    return test_stat, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94365cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python implementation of the LiNGAM algorithms.\n",
    "The LiNGAM Project: https://sites.google.com/site/sshimizu06/lingam\n",
    "\"\"\"\n",
    "#Run after previous block\n",
    "import itertools\n",
    "import warnings\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "\n",
    "\n",
    "class _BaseLiNGAM(BootstrapMixin, metaclass=ABCMeta):\n",
    "    \"\"\"Base class for all LiNGAM algorithms.\"\"\"\n",
    "\n",
    "    def __init__(self, random_state=None):\n",
    "        \"\"\"Construct a _BaseLiNGAM model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        random_state : int, optional (default=None)\n",
    "            random_state is the seed used by the random number generator.\n",
    "        \"\"\"\n",
    "        self._random_state = random_state\n",
    "        self._causal_order = None\n",
    "        self._adjacency_matrix = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, X):\n",
    "        \"\"\"Subclasses should implement this method!\n",
    "        Fit the model to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "\n",
    "    def estimate_total_effect(self, X, from_index, to_index):\n",
    "        \"\"\"Estimate total effect using causal model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Original data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        from_index :\n",
    "            Index of source variable to estimate total effect.\n",
    "        to_index :\n",
    "            Index of destination variable to estimate total effect.\n",
    "        Returns\n",
    "        -------\n",
    "        total_effect : float\n",
    "            Estimated total effect.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        X = check_array(X)\n",
    "\n",
    "        # Check from/to causal order\n",
    "        from_order = self._causal_order.index(from_index)\n",
    "        to_order = self._causal_order.index(to_index)\n",
    "        if from_order > to_order:\n",
    "            warnings.warn(\n",
    "                f\"The estimated causal effect may be incorrect because \"\n",
    "                f\"the causal order of the destination variable (to_index={to_index}) \"\n",
    "                f\"is earlier than the source variable (from_index={from_index}).\"\n",
    "            )\n",
    "\n",
    "        # from_index + parents indices\n",
    "        parents = np.where(np.abs(self._adjacency_matrix[from_index]) > 0)[0]\n",
    "        predictors = [from_index]\n",
    "        predictors.extend(parents)\n",
    "\n",
    "        # Estimate total effect\n",
    "        coefs = predict_adaptive_lasso(X, predictors, to_index)\n",
    "\n",
    "        return coefs[0]\n",
    "\n",
    "    def get_error_independence_p_values(self, X):\n",
    "        \"\"\"Calculate the p-value matrix of independence between error variables.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Original data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        independence_p_values : array-like, shape (n_features, n_features)\n",
    "            p-value matrix of independence between error variables.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        X = check_array(X)\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        E = X - np.dot(self._adjacency_matrix, X.T).T\n",
    "        p_values = np.zeros([n_features, n_features])\n",
    "        for i, j in itertools.combinations(range(n_features), 2):\n",
    "            _, p_value = hsic_test_gamma(\n",
    "                np.reshape(E[:, i], [n_samples, 1]), np.reshape(E[:, j], [n_samples, 1])\n",
    "            )\n",
    "            p_values[i, j] = p_value\n",
    "            p_values[j, i] = p_value\n",
    "\n",
    "        return p_values\n",
    "\n",
    "    def _estimate_adjacency_matrix(self, X, prior_knowledge=None):\n",
    "        \"\"\"Estimate adjacency matrix by causal order.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        prior_knowledge : array-like, shape (n_variables, n_variables), optional (default=None)\n",
    "            Prior knowledge matrix.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        if prior_knowledge is not None:\n",
    "            pk = prior_knowledge.copy()\n",
    "            np.fill_diagonal(pk, 0)\n",
    "\n",
    "        B = np.zeros([X.shape[1], X.shape[1]], dtype=\"float64\")\n",
    "        for i in range(1, len(self._causal_order)):\n",
    "            target = self._causal_order[i]\n",
    "            predictors = self._causal_order[:i]\n",
    "\n",
    "            # Exclude variables specified in no_path with prior knowledge\n",
    "            if prior_knowledge is not None:\n",
    "                predictors = [p for p in predictors if pk[target, p] != 0]\n",
    "\n",
    "            # target is exogenous variables if predictors are empty\n",
    "            if len(predictors) == 0:\n",
    "                continue\n",
    "\n",
    "            B[target, predictors] = predict_adaptive_lasso(X, predictors, target)\n",
    "\n",
    "        self._adjacency_matrix = B\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def causal_order_(self):\n",
    "        \"\"\"Estimated causal ordering.\n",
    "        Returns\n",
    "        -------\n",
    "        causal_order_ : array-like, shape (n_features)\n",
    "            The causal order of fitted model, where\n",
    "            n_features is the number of features.\n",
    "        \"\"\"\n",
    "        return self._causal_order\n",
    "\n",
    "    @property\n",
    "    def adjacency_matrix_(self):\n",
    "        \"\"\"Estimated adjacency matrix.\n",
    "        Returns\n",
    "        -------\n",
    "        adjacency_matrix_ : array-like, shape (n_features, n_features)\n",
    "            The adjacency matrix B of fitted model, where\n",
    "            n_features is the number of features.\n",
    "        \"\"\"\n",
    "        return self._adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d03c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python implementation of the LiNGAM algorithms.\n",
    "The LiNGAM Project: https://sites.google.com/site/sshimizu06/lingam\n",
    "\"\"\"\n",
    "#Run after previuos block\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "class DirectLiNGAM(_BaseLiNGAM):\n",
    "    \"\"\"Implementation of DirectLiNGAM Algorithm [1]_ [2]_\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] S. Shimizu, T. Inazumi, Y. Sogawa, A. Hyvärinen, Y. Kawahara, T. Washio, P. O. Hoyer and K. Bollen.\n",
    "       DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model.\n",
    "       Journal of Machine Learning Research, 12(Apr): 1225--1248, 2011.\n",
    "    .. [2] A. Hyvärinen and S. M. Smith. Pairwise likelihood ratios for estimation of non-Gaussian structural eauation models.\n",
    "       Journal of Machine Learning Research 14:111-152, 2013.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        random_state=None,\n",
    "        prior_knowledge=None,\n",
    "        apply_prior_knowledge_softly=False,\n",
    "        measure=\"pwling\",\n",
    "    ):\n",
    "        \"\"\"Construct a DirectLiNGAM model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        random_state : int, optional (default=None)\n",
    "            ``random_state`` is the seed used by the random number generator.\n",
    "        prior_knowledge : array-like, shape (n_features, n_features), optional (default=None)\n",
    "            Prior knowledge used for causal discovery, where ``n_features`` is the number of features.\n",
    "            The elements of prior knowledge matrix are defined as follows [1]_:\n",
    "            * ``0`` : :math:`x_i` does not have a directed path to :math:`x_j`\n",
    "            * ``1`` : :math:`x_i` has a directed path to :math:`x_j`\n",
    "            * ``-1`` : No prior knowledge is available to know if either of the two cases above (0 or 1) is true.\n",
    "        apply_prior_knowledge_softly : boolean, optional (default=False)\n",
    "            If True, apply prior knowledge softly.\n",
    "        measure : {'pwling', 'kernel'}, optional (default='pwling')\n",
    "            Measure to evaluate independence: 'pwling' [2]_ or 'kernel' [1]_.\n",
    "        \"\"\"\n",
    "        super().__init__(random_state)\n",
    "        self._Aknw = prior_knowledge\n",
    "        self._apply_prior_knowledge_softly = apply_prior_knowledge_softly\n",
    "        self._measure = measure\n",
    "\n",
    "        if self._Aknw is not None:\n",
    "            self._Aknw = check_array(self._Aknw)\n",
    "            self._Aknw = np.where(self._Aknw < 0, np.nan, self._Aknw)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit the model to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where ``n_samples`` is the number of samples\n",
    "            and ``n_features`` is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance itself.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        X = check_array(X)\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Check prior knowledge\n",
    "        if self._Aknw is not None:\n",
    "            if (n_features, n_features) != self._Aknw.shape:\n",
    "                raise ValueError(\n",
    "                    \"The shape of prior knowledge must be (n_features, n_features)\"\n",
    "                )\n",
    "            else:\n",
    "                # Extract all partial orders in prior knowledge matrix\n",
    "                if not self._apply_prior_knowledge_softly:\n",
    "                    self._partial_orders = self._extract_partial_orders(self._Aknw)\n",
    "\n",
    "        # Causal discovery\n",
    "        U = np.arange(n_features)\n",
    "        K = []\n",
    "        X_ = np.copy(X)\n",
    "        if self._measure == \"kernel\":\n",
    "            X_ = scale(X_)\n",
    "\n",
    "        for _ in range(n_features):\n",
    "            if self._measure == \"kernel\":\n",
    "                m = self._search_causal_order_kernel(X_, U)\n",
    "            else:\n",
    "                m = self._search_causal_order(X_, U)\n",
    "            for i in U:\n",
    "                if i != m:\n",
    "                    X_[:, i] = self._residual(X_[:, i], X_[:, m])\n",
    "            K.append(m)\n",
    "            U = U[U != m]\n",
    "            # Update partial orders\n",
    "            if (self._Aknw is not None) and (not self._apply_prior_knowledge_softly):\n",
    "                self._partial_orders = self._partial_orders[\n",
    "                    self._partial_orders[:, 0] != m\n",
    "                ]\n",
    "\n",
    "        self._causal_order = K\n",
    "        return self._estimate_adjacency_matrix(X, prior_knowledge=self._Aknw)\n",
    "\n",
    "    def _extract_partial_orders(self, pk):\n",
    "        \"\"\"Extract partial orders from prior knowledge.\"\"\"\n",
    "        path_pairs = np.array(np.where(pk == 1)).transpose()\n",
    "        no_path_pairs = np.array(np.where(pk == 0)).transpose()\n",
    "\n",
    "        # Check for inconsistencies in pairs with path\n",
    "        check_pairs = np.concatenate([path_pairs, path_pairs[:, [1, 0]]])\n",
    "        if len(check_pairs) > 0:\n",
    "            pairs, counts = np.unique(check_pairs, axis=0, return_counts=True)\n",
    "            if len(pairs[counts > 1]) > 0:\n",
    "                raise ValueError(\n",
    "                    f\"The prior knowledge contains inconsistencies at the following indices: {pairs[counts>1].tolist()}\"\n",
    "                )\n",
    "\n",
    "        # Check for inconsistencies in pairs without path.\n",
    "        # If there are duplicate pairs without path, they cancel out and are not ordered.\n",
    "        check_pairs = np.concatenate([no_path_pairs, no_path_pairs[:, [1, 0]]])\n",
    "        if len(check_pairs) > 0:\n",
    "            pairs, counts = np.unique(check_pairs, axis=0, return_counts=True)\n",
    "            check_pairs = np.concatenate([no_path_pairs, pairs[counts > 1]])\n",
    "            pairs, counts = np.unique(check_pairs, axis=0, return_counts=True)\n",
    "            no_path_pairs = pairs[counts < 2]\n",
    "\n",
    "        check_pairs = np.concatenate([path_pairs, no_path_pairs[:, [1, 0]]])\n",
    "        if len(check_pairs) == 0:\n",
    "            # If no pairs are extracted from the specified prior knowledge,\n",
    "            return check_pairs\n",
    "\n",
    "        pairs = np.unique(check_pairs, axis=0)\n",
    "        return pairs[:, [1, 0]]  # [to, from] -> [from, to]\n",
    "\n",
    "    def _residual(self, xi, xj):\n",
    "        \"\"\"The residual when xi is regressed on xj.\"\"\"\n",
    "        return xi - (np.cov(xi, xj)[0, 1] / np.var(xj)) * xj\n",
    "\n",
    "    def _entropy(self, u):\n",
    "        \"\"\"Calculate entropy using the maximum entropy approximations.\"\"\"\n",
    "        k1 = 79.047\n",
    "        k2 = 7.4129\n",
    "        gamma = 0.37457\n",
    "        return (1 + np.log(2 * np.pi)) / 2 - k1 * (\n",
    "            np.mean(np.log(np.cosh(u))) - gamma) ** 2 - k2 * (np.mean(u * np.exp((-(u ** 2)) / 2))) ** 2\n",
    "\n",
    "    def _diff_mutual_info(self, xi_std, xj_std, ri_j, rj_i):\n",
    "        \"\"\"Calculate the difference of the mutual informations.\"\"\"\n",
    "        return (self._entropy(xj_std) + self._entropy(ri_j / np.std(ri_j))) - (\n",
    "            self._entropy(xi_std) + self._entropy(rj_i / np.std(rj_i))\n",
    "        )\n",
    "\n",
    "    def _search_candidate(self, U):\n",
    "        \"\"\"Search for candidate features\"\"\"\n",
    "        # If no prior knowledge is specified, nothing to do.\n",
    "        if self._Aknw is None:\n",
    "            return U, []\n",
    "\n",
    "        # Apply prior knowledge in a strong way\n",
    "        if not self._apply_prior_knowledge_softly:\n",
    "            if len(self._partial_orders) != 0:\n",
    "                Uc = [i for i in U if i not in self._partial_orders[:, 1]]\n",
    "                return Uc, []\n",
    "            else:\n",
    "                return U, []\n",
    "\n",
    "        # Find exogenous features\n",
    "        Uc = []\n",
    "        for j in U:\n",
    "            index = U[U != j]\n",
    "            if self._Aknw[j][index].sum() == 0:\n",
    "                Uc.append(j)\n",
    "\n",
    "        # Find endogenous features, and then find candidate features\n",
    "        if len(Uc) == 0:\n",
    "            U_end = []\n",
    "            for j in U:\n",
    "                index = U[U != j]\n",
    "                if np.nansum(self._Aknw[j][index]) > 0:\n",
    "                    U_end.append(j)\n",
    "\n",
    "            # Find sink features (original)\n",
    "            for i in U:\n",
    "                index = U[U != i]\n",
    "                if self._Aknw[index, i].sum() == 0:\n",
    "                    U_end.append(i)\n",
    "            Uc = [i for i in U if i not in set(U_end)]\n",
    "\n",
    "        # make V^(j)\n",
    "        Vj = []\n",
    "        for i in U:\n",
    "            if i in Uc:\n",
    "                continue\n",
    "            if self._Aknw[i][Uc].sum() == 0:\n",
    "                Vj.append(i)\n",
    "        return Uc, Vj\n",
    "\n",
    "    def _search_causal_order(self, X, U):\n",
    "        \"\"\"Search the causal ordering.\"\"\"\n",
    "        Uc, Vj = self._search_candidate(U)\n",
    "        if len(Uc) == 1:\n",
    "            return Uc[0]\n",
    "\n",
    "        M_list = []\n",
    "        for i in Uc:\n",
    "            M = 0\n",
    "            for j in U:\n",
    "                if i != j:\n",
    "                    xi_std = (X[:, i] - np.mean(X[:, i])) / np.std(X[:, i])\n",
    "                    xj_std = (X[:, j] - np.mean(X[:, j])) / np.std(X[:, j])\n",
    "                    ri_j = (\n",
    "                        xi_std\n",
    "                        if i in Vj and j in Uc\n",
    "                        else self._residual(xi_std, xj_std)\n",
    "                    )\n",
    "                    rj_i = (\n",
    "                        xj_std\n",
    "                        if j in Vj and i in Uc\n",
    "                        else self._residual(xj_std, xi_std)\n",
    "                    )\n",
    "                    M += np.min([0, self._diff_mutual_info(xi_std, xj_std, ri_j, rj_i)]) ** 2\n",
    "            M_list.append(-1.0 * M)\n",
    "        return Uc[np.argmax(M_list)]\n",
    "\n",
    "    def _mutual_information(self, x1, x2, param):\n",
    "        \"\"\"Calculate the mutual informations.\"\"\"\n",
    "        kappa, sigma = param\n",
    "        n = len(x1)\n",
    "        X1 = np.tile(x1, (n, 1))\n",
    "        K1 = np.exp(-1 / (2 * sigma ** 2) * (X1 ** 2 + X1.T ** 2 - 2 * X1 * X1.T))\n",
    "        X2 = np.tile(x2, (n, 1))\n",
    "        K2 = np.exp(-1 / (2 * sigma ** 2) * (X2 ** 2 + X2.T ** 2 - 2 * X2 * X2.T))\n",
    "\n",
    "        tmp1 = K1 + n * kappa * np.identity(n) / 2\n",
    "        tmp2 = K2 + n * kappa * np.identity(n) / 2\n",
    "        K_kappa = np.r_[np.c_[tmp1 @ tmp1, K1 @ K2], np.c_[K2 @ K1, tmp2 @ tmp2]]\n",
    "        D_kappa = np.r_[\n",
    "            np.c_[tmp1 @ tmp1, np.zeros([n, n])], np.c_[np.zeros([n, n]), tmp2 @ tmp2]\n",
    "        ]\n",
    "\n",
    "        sigma_K = np.linalg.svd(K_kappa, compute_uv=False)\n",
    "        sigma_D = np.linalg.svd(D_kappa, compute_uv=False)\n",
    "\n",
    "        return (-1 / 2) * (np.sum(np.log(sigma_K)) - np.sum(np.log(sigma_D)))\n",
    "\n",
    "    def _search_causal_order_kernel(self, X, U):\n",
    "        \"\"\"Search the causal ordering by kernel method.\"\"\"\n",
    "        Uc, Vj = self._search_candidate(U)\n",
    "        if len(Uc) == 1:\n",
    "            return Uc[0]\n",
    "\n",
    "        if X.shape[0] > 1000:\n",
    "            param = [2e-3, 0.5]\n",
    "        else:\n",
    "            param = [2e-2, 1.0]\n",
    "\n",
    "        Tkernels = []\n",
    "        for j in Uc:\n",
    "            Tkernel = 0\n",
    "            for i in U:\n",
    "                if i != j:\n",
    "                    ri_j = (\n",
    "                        X[:, i]\n",
    "                        if j in Vj and i in Uc\n",
    "                        else self._residual(X[:, i], X[:, j])\n",
    "                    )\n",
    "                    Tkernel += self._mutual_information(X[:, j], ri_j, param)\n",
    "            Tkernels.append(Tkernel)\n",
    "\n",
    "        return Uc[np.argmin(Tkernels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7456195",
   "metadata": {},
   "source": [
    "### Create prior knowledge - removes edges from graph which cannot exist due to temporality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26443257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create prior knowledge for algorithm Lingam -partial prior grapth - constraints which cannot exist in reality.\n",
    "#Works for collider as well - will remove from Lingam edges which cannot exist due to temporality\n",
    "def get_prior(new_df):\n",
    "    prior_knowledge = np.eye(len(new_df.columns)) -1\n",
    "    for ind1, column1 in enumerate(new_df.columns):\n",
    "        for ind2, column2 in enumerate(new_df.columns):\n",
    "            if column1!=column2 :\n",
    "                residuals = new_df[column1] - new_df[column2]\n",
    "                if (residuals >= 0).all():\n",
    "                    #print(f'from {column1} to {column2} {np.where(residuals >= 0)[0]}')\n",
    "                    prior_knowledge[ind2][ind1] = 0\n",
    "    return prior_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "890261a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relevant usage\n",
    "#prior_knowledge = get_prior(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143d43b9",
   "metadata": {},
   "source": [
    "### LiNGAM invocation without prior knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f8288d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lingam.direct_lingam.DirectLiNGAM at 0x20d445c4e80>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run it first time and then create the graph\n",
    "#In the second run , run the new Lingam and create the graph\n",
    "\n",
    "############################\n",
    "# Yuval run the old LINGAM , and directly after that run the block for visualization of the graph created by this algorithm-\n",
    "# the |make_dot(model.adjacency_matrix_, labels = list(new_df.columns)) block\n",
    "# Afterwards he run the new LINGAM and again, the code block for visualization - the code block with make_dot function\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "reg = RandomForestRegressor(max_depth=10, random_state=0)\n",
    "reg = Ridge()\n",
    "reg = LinearRegression()\n",
    "\n",
    "model = lingam.DirectLiNGAM()\n",
    "#uncomment for invocation on df\n",
    "#model.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace4e5dc",
   "metadata": {},
   "source": [
    "### LiNGAM with prior knowledge invocation example - with restrications on coefficents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f05d174",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DirectLiNGAM at 0x20d445c49d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "import inspect\n",
    "reg = RandomForestRegressor(max_depth=10, random_state=0)\n",
    "reg = Ridge()\n",
    "reg = LinearRegression()\n",
    "\n",
    "model = DirectLiNGAM(prior_knowledge=prior_knowledge)\n",
    "\n",
    "\n",
    "#model.fit(new_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
